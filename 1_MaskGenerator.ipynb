{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preaparation of the dataset\n",
    "\n",
    "The given dataset of crosswalking islands consits of over 4000 images saved as .tif files as well as a CSV file that contains polygon coordinates. These coordinates are needed for the creation of the binary masks which are not included in the dataset. Furthermore, there exist also invalid images or damaged images in the set. Thus, it must be cleaned before the creation of the mask. This notebook solves the following problems in chronological order:\n",
    "\n",
    "- Download of dataset from SwitchDrive and unzpiing of content in current directory\n",
    "- Conversion of large TIF files in smaller PNG files in separate directory\n",
    "- Data cleaning\n",
    "- Creation of labels in the form of binary masks\n",
    "- Additional data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "import tifffile\n",
    "import zipfile\n",
    "import random\n",
    "import json\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastai.vision.all import *\n",
    "from skimage import io, exposure\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgba2rgb\n",
    "from skimage.util import img_as_ubyte\n",
    "from multiprocessing import Pool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions used in this notebook are stored in a dedicated python function file. This has the aim to make the structure clearer and reader-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import created functions from separate python file defs.py (same directory)\n",
    "from defs import read_image, reduce_image_array, save_img_as_png, split_list, convert_list_of_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surpresses OS warnings when apply multiprocessing later\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working directory\n",
    "Lets start with the creation of the working directory for the needed data. The code below creates a new data folder at the current working directory. Inside that folder, a subdirectory for each, TIF images of refuge-island and the corresponding masks are created. If, however, the directory already exists, this step is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = Path(os.getcwd()) / \"data\"\n",
    "IMAGES_PATH = DATASET_PATH / \"islands\"\n",
    "MASKS_PATH = DATASET_PATH  / \"masks\"\n",
    "\n",
    "# Create Directory\n",
    "if not DATASET_PATH.exists():\n",
    "    os.mkdir(DATASET_PATH)\n",
    "if not MASKS_PATH.exists():\n",
    "    os.mkdir(MASKS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TIF images and convert them to PNG format\n",
    "The whole dataset (images and csv file) can be downloaded as zip file. Due to the large size of the .tif images, they are not suitable for the further image segmentation process. Therefore, each image is converted (and compressed) to a .png in a separate directory.\n",
    "\n",
    "> This takes around 10-20 min and requires a constant internet connection. To avoid doing this step multiple time it is checked whether the path with the images already exists before initiating the download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Images and CSV file as Zip\n",
    "if not IMAGES_PATH.exists():    \n",
    "    urllib.request.urlretrieve(url=r'https://drive.switch.ch/index.php/s/bWb8D4mQe7HpSqL/download',filename=DATASET_PATH/'islands.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip images \n",
    "if not IMAGES_PATH.exists():\n",
    "    with zipfile.ZipFile(DATASET_PATH/'islands.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATASET_PATH)\n",
    "    with zipfile.ZipFile(DATASET_PATH/'image-segmentation-islands'/'islands.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multiprocessing to speed up the conversion process\n",
    "\n",
    "Since there are over 4000 images that must be converted, multiprocessing is used to fully leverage the processor capabilities of the used machine. During this process, the images are grouped in multiple sets and are compressed to a smaller scale before saved as PNG images.\n",
    "\n",
    "> Still, this process can take about 30+ mins. Depending on the capability of the used machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "# Create png folder\n",
    "IMAGES_PNG_PATH = DATASET_PATH / 'images_png'\n",
    "if not IMAGES_PNG_PATH.exists():\n",
    "    os.mkdir(IMAGES_PNG_PATH)\n",
    "    \n",
    "    number_of_parallelism = 10\n",
    "    # Get images in a list []\n",
    "    image_list = list(IMAGES_PATH.glob(\"*.tif\"))\n",
    "\n",
    "    # Split list into n sublist [[][]]\n",
    "    image_list_splitted = split_list(image_list, number_of_parallelism)\n",
    "    do_conversion = partial(convert_list_of_images, out_folder = IMAGES_PNG_PATH)\n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        with Pool(number_of_parallelism) as p:\n",
    "            print(p.map(do_conversion, image_list_splitted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of CSV file \n",
    "\n",
    "The CSV file comprises of the following columns:\n",
    "\n",
    "- Image path (reference to which image file the information is applicable)\n",
    "- The polygon coordinates\n",
    "- A boolean value whether the image is valid or not\n",
    "\n",
    "> Invalid images must be exluded from the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = IMAGES_PATH / \"validated.csv\"\n",
    "\n",
    "data_list = []\n",
    "\n",
    "with open (file_path, 'r', encoding=\"utf-8-sig\") as f_obj:\n",
    "    reader = csv.DictReader(f_obj, delimiter=',')\n",
    "    for item in reader:\n",
    "        data_list.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cleaning process, the data of the CSV file is transfered into a Pandas dataframe. Rows containing invalid images are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [['image_path','polygons', 'invalid_image']]\n",
    "for record in data_list:  \n",
    "    image_path = record['path']\n",
    "    polygons = record['polygons']\n",
    "    invalid_image = record['invalid_image']\n",
    "    \n",
    "    columns.append([image_path, polygons, invalid_image])\n",
    "    \n",
    "    df = pd.DataFrame(columns[1:], columns=columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda x: x.replace({'.tif': '.png', '/data/islands/': '',}, regex=True, inplace = True))\n",
    "df['polygons'] = df['polygons'].str[1:-1]\n",
    "df = df[df.invalid_image == 'False']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4202"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of valid images: \" + df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask generation\n",
    "\n",
    "Now that the CSV file is cleaned, the image list with the assigned polygon coordinates can be used to create the binary masks. There is a dedicated method from the OpenCV (cv2) library that fulfills this job.\n",
    "\n",
    "> Note that this is done with the png images, not with the tif images\n",
    "\n",
    "Afterwards, the number of erroneous files are acertained. That can have the following two reasons:\n",
    "\n",
    "- Polygon coordinates are empty or in the wrong format (invalid_images)\n",
    "- The image path of the CSV file does not match with any image file of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "not_matching_images = []\n",
    "invalid_images = []\n",
    "\n",
    "#df = df.reset_index()\n",
    "for index, row in df.iterrows():\n",
    "    path = str(IMAGES_PNG_PATH / (row['image_path']))\n",
    "    if os.path.isfile(path):\n",
    "        try:\n",
    "            img = cv2.imread(path)\n",
    "            image = np.zeros((img.shape[0], img.shape[1]))\n",
    "            # Extract and allocate coordinates for mask generation\n",
    "            polygon = np.asarray(json.loads(row['polygons']), dtype=np.float32) #np.array(row['polygons'])\n",
    "            contours = polygon.astype(int)\n",
    "            mask = cv2.fillPoly(image, pts = contours, color =(255,255,255))\n",
    "            cv2.imwrite(str(MASKS_PATH / (row['image_path'])),mask)\n",
    "        except:\n",
    "            invalid_images.append(str(row['image_path']))\n",
    "            os.remove(path)\n",
    "    else:\n",
    "        not_matching_images.append(path)\n",
    "            \n",
    "print(len(invalid_images))\n",
    "print(len(not_matching_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step, the images which are not included in the the validated.csv file must be deleted (they are of no use since there are no labels available for the training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "png_list = []\n",
    "mask_list = []\n",
    "\n",
    "for filename in os.listdir(IMAGES_PNG_PATH):\n",
    "    png_list.append(filename)\n",
    "    \n",
    "for filename in os.listdir(MASKS_PATH):\n",
    "    mask_list.append(filename)\n",
    "    \n",
    "difference = list(set(png_list)-set(mask_list))\n",
    "print(len(difference))\n",
    "\n",
    "for file in difference:\n",
    "    os.remove(IMAGES_PNG_PATH / str(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark\n",
    "\n",
    "In hindsight, it might make more sense to delete to unusable images first before the conversion process. This would save computation time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
